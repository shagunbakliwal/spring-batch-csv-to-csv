Here's an updated solution for reading multiple fixed-length files from a directory, determining the number of workers and partitions based on line counts, parsing the data with different mappers based on the file content, and handling errors by writing failed file content to another file. This includes appropriate listeners and exception handling.

Requirements Breakdown:

1. Multiple fixed-length files: Read from a directory.


2. Line count-based partitioning: Dynamically decide the number of workers/partitions based on line counts.


3. Dynamic mappers: Choose a different mapper based on the file content.


4. Database persistence: Save the data into a database using Spring Data JPA.


5. Error handling: In case of failure, log the error and write the failed fileâ€™s content to another file.


6. Listeners: To manage job/step lifecycle and error logging.




---

1. Custom Partitioner for Line Counting

The custom partitioner splits the work based on the number of lines in each file, determining the grid size dynamically.

import org.springframework.batch.core.partition.support.MultiResourcePartitioner;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.core.io.Resource;

import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.Map;

public class LineCountingPartitioner extends MultiResourcePartitioner {

    @Override
    public Map<String, ExecutionContext> partition(int gridSize) {
        Resource[] resources = getResources();
        Map<String, ExecutionContext> partitionMap = new HashMap<>();

        for (int i = 0; i < resources.length; i++) {
            Resource resource = resources[i];

            try {
                int lineCount = countLinesInFile(resource);
                int partitions = calculatePartitions(lineCount, gridSize);

                for (int j = 0; j < partitions; j++) {
                    ExecutionContext context = new ExecutionContext();
                    context.putString("fileName", resource.getFile().getAbsolutePath());
                    context.putInt("startLine", j * (lineCount / partitions));
                    context.putInt("endLine", Math.min(lineCount, (j + 1) * (lineCount / partitions)) - 1);
                    partitionMap.put("partition" + i + "-" + j, context);
                }
            } catch (Exception e) {
                throw new RuntimeException("Error partitioning file: " + resource.getFilename(), e);
            }
        }

        return partitionMap;
    }

    private int countLinesInFile(Resource resource) throws Exception {
        BufferedReader reader = new BufferedReader(new InputStreamReader(resource.getInputStream()));
        int lines = 0;
        while (reader.readLine() != null) lines++;
        reader.close();
        return lines;
    }

    private int calculatePartitions(int lineCount, int gridSize) {
        // Partition based on line count, e.g., 1000 lines per partition, max gridSize workers.
        return Math.min(lineCount / 1000 + 1, gridSize);
    }
}

2. FlatFileItemReader with Dynamic Mapper Selection

This part dynamically chooses a LineMapper based on the file content.

@Bean
@StepScope
public FlatFileItemReader<Object> customFlatFileItemReader(
        @Value("#{stepExecutionContext['fileName']}") String fileName,
        @Value("#{stepExecutionContext['startLine']}") int startLine,
        @Value("#{stepExecutionContext['endLine']}") int endLine) {

    FlatFileItemReader<Object> reader = new FlatFileItemReader<>();
    reader.setResource(new FileSystemResource(fileName));

    // Determine mapper dynamically based on file content
    reader.setLineMapper(determineMapperBasedOnFileContent(fileName));

    // Skip already processed lines
    reader.setLinesToSkip(startLine);

    return reader;
}

private LineMapper<Object> determineMapperBasedOnFileContent(String fileName) {
    // Example logic: Choose the mapper based on file name or file content
    if (fileName.contains("FormatA")) {
        return fixedLengthMapperA();
    } else if (fileName.contains("FormatB")) {
        return fixedLengthMapperB();
    } else {
        throw new IllegalArgumentException("Unknown format for file: " + fileName);
    }
}

@Bean
public LineMapper<Object> fixedLengthMapperA() {
    FixedLengthTokenizer tokenizer = new FixedLengthTokenizer();
    tokenizer.setNames("field1", "field2");
    tokenizer.setColumns(new Range(1, 10), new Range(11, 20));

    DefaultLineMapper<Object> lineMapper = new DefaultLineMapper<>();
    lineMapper.setLineTokenizer(tokenizer);
    lineMapper.setFieldSetMapper(new BeanWrapperFieldSetMapper<>() {{
        setTargetType(MyEntityA.class);
    }});

    return lineMapper;
}

@Bean
public LineMapper<Object> fixedLengthMapperB() {
    FixedLengthTokenizer tokenizer = new FixedLengthTokenizer();
    tokenizer.setNames("fieldA", "fieldB");
    tokenizer.setColumns(new Range(1, 15), new Range(16, 25));

    DefaultLineMapper<Object> lineMapper = new DefaultLineMapper<>();
    lineMapper.setLineTokenizer(tokenizer);
    lineMapper.setFieldSetMapper(new BeanWrapperFieldSetMapper<>() {{
        setTargetType(MyEntityB.class);
    }});

    return lineMapper;
}

3. Partition Step

Configure the partition step to divide the processing across workers.

@Bean
public Step partitionStep(StepBuilderFactory stepBuilderFactory, 
                          LineCountingPartitioner partitioner, 
                          Step workerStep) {
    return stepBuilderFactory.get("partitionStep")
            .partitioner("workerStep", partitioner)
            .step(workerStep)
            .gridSize(10)  // Number of workers
            .taskExecutor(new SimpleAsyncTaskExecutor())
            .build();
}

4. Worker Step

The worker step processes each partition (file chunk) and writes parsed data into the database.

@Bean
public Step workerStep(StepBuilderFactory stepBuilderFactory, 
                       FlatFileItemReader<Object> customFlatFileItemReader, 
                       ItemWriter<Object> itemWriter, 
                       StepExecutionListener stepListener) {
    return stepBuilderFactory.get("workerStep")
            .<Object, Object>chunk(100)  // Process in chunks
            .reader(customFlatFileItemReader)
            .writer(itemWriter)
            .listener(stepListener)  // Attach listeners
            .faultTolerant()
            .skip(Exception.class)  // Skip and log failed records
            .skipLimit(10)          // Limit the number of skipped items
            .build();
}

5. ItemWriter for Database Storage

Use Spring Data JPA to persist the parsed data into the database.

@Bean
public ItemWriter<Object> itemWriter(MyEntityRepository repository) {
    return items -> repository.saveAll(items);
}

6. Exception Handling and Listeners

In case of failure, the listener writes the failed file content to another file for logging and debugging.

import org.springframework.batch.core.listener.StepExecutionListenerSupport;
import org.springframework.batch.core.StepExecution;
import org.springframework.batch.core.ExitStatus;
import org.springframework.stereotype.Component;
import java.io.*;

@Component
public class ErrorLoggingStepListener extends StepExecutionListenerSupport {

    @Override
    public ExitStatus afterStep(StepExecution stepExecution) {
        if (stepExecution.getExitStatus().equals(ExitStatus.FAILED)) {
            logFailedFile(stepExecution.getJobParameters().getString("fileName"));
        }
        return super.afterStep(stepExecution);
    }

    private void logFailedFile(String fileName) {
        try {
            File failedFile = new File("failed_files", new File(fileName).getName());
            FileWriter writer = new FileWriter(failedFile, true);
            writer.write("Processing failed for file: " + fileName + "\n");
            writer.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

7. Job Configuration

Finally, configure the job, including the partition and worker steps, along with listeners for error handling.

@Bean
public Job partitionedJob(JobBuilderFactory jobBuilderFactory, 
                          Step partitionStep, 
                          StepExecutionListener stepListener) {
    return jobBuilderFactory.get("partitionedJob")
            .listener(stepListener)  // Add job-level listeners if needed
            .start(partitionStep)
            .build();
}


---

Summary

1. MultiResourcePartitioner: Dynamically partitions files based on line counts and distributes them across multiple workers.


2. Dynamic Mapper Selection: Chooses different mappers based on the file content, allowing different formats to be processed in parallel.


3. Listeners & Error Handling: Errors during processing are logged, and failed file contents are written to a separate file.


4. Grid Size & Parallelism: The number of partitions and workers is dynamically adjusted based on the file size and line counts.


5. Database Storage: Parsed data is stored in the database using Spring Data JPA.


6. Logging Failures: If an error occurs, the content of the failed file is logged for review.



This approach ensures efficient, parallel processing of large fixed-length files while handling various formats and managing errors gracefully.

